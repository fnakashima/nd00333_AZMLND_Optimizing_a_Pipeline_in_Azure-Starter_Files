# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains UCI Bank Marketing data. This is a classification problem and the goal is to predict if the client will subscribe to a term deposit with the bank.
The best performing model was a model generated by AutoML and the accuracy was 0.92098.

## Scikit-learn Pipeline
The Azure ML pipeline consists the following steps.
- Creating a compute cluster
- Providing a parameter sampler
- Providing an early stopping policy
- Providing a HyperDriveConfig
- Submitting the HyperDriveConfig to run a train script
- Retrieving the best run to select the best hyperparameters
- Saving the best model

In the train script, the bank marketing data is retrieved from the URL specified and clearned for training, and splitted into train (75%) and test dataset (25%).

This time, a Scikit-learn [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) was used as a classification algorithm and two hyperparameters are selected, one is the inverse of regularization strength(C) and another is the maximum number of iterations to converge(max_iter).

In terms of parameter sampling, [Random Parameter Sampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?view=azure-ml-py) was used.
The random sampling supports early termination of low performance runs, therefore, we can save time for training and cost for computing resource and this is good especially for the initial search.
This time, the choice of 6 parameters for C, and the choice of 4 parameters are applied.

Regarding an early termination policy, [Bandit Policy](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py) was applied.
This policy ends runs when the primary metric isn't withing the specified slack factor/amount of the most successful run.

## AutoML
The AutoML model was simply defined by AutoMLConfig and optimized automatically.
This time, accuracy was specified as a primary metric as it was also used by HyperDrive.
n_cross_validations was 10 which is commonly used. In addition, validating with splitted test dataset in the same way as HyperDrive training was also applied to compare.
In the result, validating with test dataset outperformed by looking at accurary. (n_cross_validations: 0.91737, validating by test data: 0.92098)

## Pipeline comparison
In this project, the model generated by AutoML showed a better performance compared with the ones optimized by HyperDrive.
AutoML models shows 0.92098 accuracy while the best accuracy of HyperDrive model was 0.91648.
The result of HyperDrive optimatization looks unstable. This may be because of Random Paramter Sampling with multiple choice.
However, the time for training of HyperDrive (less than 10 mins) was faster than AutoML (30 mins).

## Future work
In random parameter sampling, it might be better to use continuous hyperparameters as the results were pretty rondom.
In aiddtion, the parameters of early stopping policy can also be tuned.
